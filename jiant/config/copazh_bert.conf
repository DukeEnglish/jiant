// An example configuration for the CommitmentBank task with BERT.
// Run with:
//   python main.py --config_file jiant/config/cb_bert.conf

// This imports the defaults, which can be overridden below.
include "defaults.conf"

// Basics
exp_name = copa_with_bert
list_params = 1  // Quieter logs, since we're not experimenting with new or exciting architectures.
write_preds = test  // Write test set predictions to disk for use on SuperGLUE if desired.

// Standard setup for training on a single target task
pretrain_tasks = copa
target_tasks = copa
// If you would like to simply multitask train on tasks, then simply set do_pretrain=1 and target_task_training = 0.
// If you would like to train an encoder on a set of tasks sequentially without sharing the encoder, set do_pretrain=0 and target_task_training = 1.
// If you would like to evaluate on some tasks, based on a previously jiant trained model, set load_eval_checkpoint to the path of that model checkpoint, and then set do_pretrain=0, do_target_task_training=0, do_full_eval=1.

do_pretrain = 1
do_target_task_training = 0
do_full_eval = 1

// Typical BERT base setup
chinese = zh
flag_downloaded_model=01
input_module = bert-base-chinese // xlnet-base-cased // bert-base-chinese //Users/ljy/Work/Reseach_academy/github/jiant/bert-zh-pytorch/  //bert-base-chinese // bert-base-uncased
transfer_paradigm = finetune
classifier = log_reg
optimizer = bert_adam
lr = 0.00001
sent_enc = none
sep_embs_for_skip = 1
max_seq_len = 510
dropout = 0.1

// Trainer setup for small tasks with BERT
val_interval = 10
batch_size = 12
max_epochs = 4


